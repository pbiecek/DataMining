---
title: "hw6"
output: html_document
---

```{r setup, include=FALSE}

library(mlbench)
library(e1071)
library(caret)

library(ROCR)
library(klaR)

library(randomForest)
library(pROC)
data(Soybean)
```

# Soybean class dataset

Soybean dataset contains variables that describe number of properties of given bean. Damage, color, leaves etc.
All of the variables are of range 0-4 or less. They also contain NaN's (as missing values). They are classified with descriptive name, like "brown-stem-rot", "cyst-nematode" describing type of damage.

Here's some statisctics:

```{r}
dim(Soybean)
levels(Soybean$Class)
head(Soybean)
summary(Soybean)
```

All of the variables are Factors, so it is neccessary to change some to numeric, like this:

```{r}
for (i in 2:36) {
  Soybean[,i] <- as.numeric(Soybean[,i])
}
```

## Modeling

It's quite difficult dataset, as most visualisation do not show meaningfull results, like this plot

```{r}
svm.model=svm(Class~., data=Soybean, kernel="polynomial",probability=FALSE)

plot(svm.model, Soybean, canker.lesion ~ stem.cankers,
     svSymbol = "x", dataSymbol = "o", symbolPalette = rainbow(4),
     color.palette = terrain.colors,fill=TRUE,grid=10)
```

This is due to zero variance per class for most of variables. This means that choosing single class to measure variance, for most variables it is zero. That does not mean that variables do not vary, between classes they do, mostly.

We remove few zero variance variables to lessen the model.

```{r}
nzv <- nearZeroVar(Soybean)
Soybean <- Soybean[-nzv]
Soybean[is.na(Soybean)] <- 0

lvls = levels(Soybean$Class)
testidx = which(1:length(Soybean[, 1]) %% 2 == 0) 
Soybean.train = Soybean[testidx, ]
Soybean.test = Soybean[-testidx, ]
```

## NaiveBayes

Naive bayes fares pretty well for most of the classes, some are predicted properly almost in 100%! To visualise let's draw 1-to-all roc curves per each class:

```{r}
aucs = c()
plot.new()
plot(x=NA, y=NA, xlim=c(0,1), ylim=c(0,1),
     ylab='True Positive Rate',
     xlab='False Positive Rate',
     bty='n')

for (type.id in 1:19) {
  type = as.factor(Soybean.train$Class == lvls[type.id])
  
  nbmodel = naiveBayes(type ~ ., data=Soybean.train[,2:33])
  nbprediction = predict(nbmodel, Soybean.test[,2:33], type='raw')
  
  score = nbprediction[, 'TRUE']
  actual.class = Soybean.test$Class == lvls[type.id]
  
  pred = prediction(score, actual.class)
  nbperf = performance(pred, "tpr", "fpr")
  
  roc.x = unlist(nbperf@x.values)
  roc.y = unlist(nbperf@y.values)
  lines(roc.y ~ roc.x, col=type.id+1, lwd=2)
  
  nbauc = performance(pred, "auc")
  nbauc = unlist(slot(nbauc, "y.values"))
  aucs[type.id] = nbauc
}
lines(x=c(0,1), c(0,1))
```

And a multiclass roc result:

```{r}
mean(aucs)
library(randomForest)
library(pROC)
rf = randomForest(Class~., data = Soybean, ntree = 100)
pr = predict(rf, Soybean, type = 'prob')
myPr <- sapply(1:nrow(Soybean), function(i){
  pr[i, Soybean$Class[i]]
})
multiclass.roc(Soybean$Class, myPr)
```

## Predicting

Prediction requires high number of training observations, because many predictors do not fare well with zero variance variables ans, as with naive bayes in this example, they can even fail with error if the data is too few.

## Conclusion

There are a lot of classes. Data does not require excessive preparation steps, so it is good for new users that wish to learn about prediction. Since they can start with only classification and do not need to do data preparation steps, it will be good learing example for data sampling and prediction models.


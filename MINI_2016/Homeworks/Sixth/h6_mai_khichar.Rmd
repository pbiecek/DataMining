---
title: "Multi-Label Classification"
author: "Viet Ba Mai, Shilpa Khichar"
date: "10 listopada 2016"
output: 
  html_document:
    toc: TRUE
---


#Multi-class classification
Most of classifiers are meant for binary classification.
Below is a tutorial on how one can perform it having multiple classes.

#Data
The dataset was collected via National Indonesia Contraceptive Prevalence Survey. The features describe demographic and socio-economic characteristics of women that were either not pregnant or not aware of their pregnancy when interviewed. The target variable is `Contraceptive_Method` and represents 3 classes:

- [1] No use

- [2] Long-term methods

- [3] Short-term methods


#Data preparation and analysis
```{r, warning = FALSE, message = FALSE, echo=TRUE, fig.align='center'}
library(caret)
library(ROCR)
library(klaR)
library(randomForest)
library(pROC)
library(corrplot)
library(magrittr)
library(plyr)
library(dplyr)
library(ggplot2)
library(e1071)

data <- read.csv(file="C:/Users/vietba/Downloads/contraceptive.csv", sep=",", encoding="UTF-8")
summary(data)

data$Contraceptive_method <- factor(data$Contraceptive_method)
levels(data$Contraceptive_method)
summary(data$Contraceptive_method)

barplot(table(data$Contraceptive_method), col="lightgreen", main="Distribution of Contraceptive_method")

pairs(data[1:5], pch = 21, bg = c("red", "green3","blue")[unclass(data$Contraceptive_method)])
pairs(data[6:9], pch = 21, bg = c("red", "green3","blue")[unclass(data$Contraceptive_method)])
boxplot(Children~Contraceptive_method, data=data)

set.seed(1313)
indxTrain <- createDataPartition(y = data$Contraceptive_method, p = 0.75)
str(indxTrain)

train <- data[indxTrain$Resample1,]
test <- data[-indxTrain$Resample1,]

summary(train$Contraceptive_method)
summary(test$Contraceptive_method)
```

#SVM
Support Vector Machines are models applicable for binary tasks.
However function `svm()` from library `e1071` can perform a multi-class classification directly.

The idea is that it permorms a **one-against-1one** classification - it trains binary classifiers for all pair combinations of target classes and the result is obtained by calculating the most voted classifier.
```{r, warning = FALSE, message = FALSE, echo=TRUE, fig.align='center'}
svm.model=svm(Contraceptive_method~., data=train, kernel="polynomial",probability=TRUE)
svm.model

plot(svm.model, train, Children ~ Wife_age,
     slice = list(Wife_education = 0, Husband_education = 1),
     svSymbol = "x", dataSymbol = "o", symbolPalette = rainbow(2),
     color.palette = terrain.colors,fill=TRUE)

pred <- predict(svm.model, test, decision.values = TRUE, probability = TRUE)
predSVM <- predict(svm.model, test)
svmAUC <- auc(predSVM, as.numeric(test$Contraceptive_method))
plot.roc(predSVM, as.numeric(test$Contraceptive_method), col="green")

summary(pred)

head(attr(pred, "decision.values"))
head(attr(pred, "probabilities"))

# visualize (classes by color, SV by crosses):
#Multidimensional scaling takes a set of dissimilarities and returns a set of points such that the distances between the points are approximately equal to the dissimilarities.
plot(cmdscale(dist(test[,-10])),
     col = as.integer(test[,10]),
     pch = c("o","+")[1:150 %in% svm.model$index + 1])


mydf = cbind(test, pred)
qplot(Wife_age, Children, colour = Contraceptive_method, shape = pred, data = test)
```

#Random Forest
This is one of the most powerful learning tools and it is capable of performing classification on a large number of classes.
There are many other such classifiers and using them can be the simplest approach since they can be applied directly on multiple classes.

Multiclass `ROC` curve can be plot with a prediction vector being calculated by traversing over the Random Forest's prediction probabilities.
```{r, warning = FALSE, message = FALSE, echo=TRUE, fig.align='center'}
rf = randomForest(Contraceptive_method ~ ., data = train, ntree = 100)
pr = predict(rf, test, type = 'prob')
myPr <- sapply(1:nrow(test), function(i){
  pr[i, test$Contraceptive_method[i]]
})
multiclass.roc(test$Contraceptive_method, myPr)

```

#Naive Bayes
We can check performance for each type in the class levels.
Below `Area Under Curve` is calculated for classification done with `Naive Bayes` model (which is also a multi-class method).
```{r, warning = FALSE, message = FALSE, echo=TRUE, fig.align='center'}
lvls = levels(train$Contraceptive_method)
aucs = c()
plot(x=NA, y=NA, xlim=c(0,1), ylim=c(0,1),
     ylab='True Positive Rate',
     xlab='False Positive Rate',
     bty='n')

for (type.id in 1:3) {
  type.id
  type = as.factor(train$Contraceptive_method == lvls[type.id])

  nbmodel = NaiveBayes(type ~ ., data=train[, -10])
  nbprediction = predict(nbmodel, test[,-10], type='raw')

  score = nbprediction$posterior[, 'TRUE']
      actual.class = test$Contraceptive_method == lvls[type.id]

  pred = prediction(score, actual.class)
  nbperf = performance(pred, "tpr", "fpr")

  roc.x = unlist(nbperf@x.values)
  roc.y = unlist(nbperf@y.values)
  lines(roc.y ~ roc.x, col=type.id+1, lwd=2)

  nbauc = performance(pred, "auc")
  nbauc = unlist(slot(nbauc, "y.values"))
  aucs[type.id] = nbauc
}

lines(x=c(0,1), c(0,1))

mean(aucs)
```

#Conclusions
There are many methods to solve multiclass classifications. An obvious choice would be to apply classifiers which can train on multiple classes directly (such as `Naive Bayes` and `Random Forest`).

Another method would be to do a **one-against-one** approach described above. A different, but generally giving worse results option, is to compare one class against all the other classes together (**one-against-all**).

In our problem the best result was obtained with `SVM` (which implements one-against-one) with the AUC value of ~0.71 (the closest it is to 1.0 the better).

<!-- #Multi-Class Summary Function -->
<!-- Based on caret:::twoClassSummary, implementation taken from: https://www.r-bloggers.com/error-metrics-for-multi-class-problems-in-r-beyond-accuracy-and-kappa/ -->
<!-- ```{r, warning = FALSE, message = FALSE, echo=TRUE} -->
<!-- library(Metrics) -->
<!-- library(pROC) -->
<!-- require(compiler) -->
<!-- multiClassSummary <- cmpfun(function (data, lev = NULL, model = NULL){ -->

<!--   #Load Libraries -->
<!--   #(Metrics) -->
<!--   #library(caret) -->
<!--   #library(pROC) -->
<!--   #library(Metrics) -->
<!--   #Calculate custom one-vs-all stats for each class -->
<!--   prob_stats <- lapply(levels(data[, "pred"]), function(class){ -->
<!--     class -->
<!--     #Grab one-vs-all data for the class -->
<!--     pred <- ifelse(data[, "pred"] == class, 1, 0) -->
<!--     pred -->
<!--     obs  <- ifelse(data[,  "obs"] == class, 1, 0) -->
<!--     obs -->
<!--     prob <- data[,class] -->
<!--     prob -->
<!--     #Calculate one-vs-all AUC and logLoss and return -->
<!--     cap_prob <- pmin(pmax(prob, .000001), .999999) -->
<!--     prob_stats <- c(auc(obs, prob), logLoss(obs, cap_prob)) -->
<!--     names(prob_stats) <- c('ROC', 'logLoss') -->
<!--     return(prob_stats)  -->
<!--   }) -->
<!--   prob_stats <- do.call(rbind, prob_stats) -->
<!--   rownames(prob_stats) <- paste('Class:', levels(data[, "pred"])) -->

<!--   #Calculate confusion matrix-based statistics -->
<!--   CM <- confusionMatrix(data[, "pred"], data[, "obs"]) -->

<!--   #Aggregate and average class-wise stats -->
<!--   #Todo: add weights -->
<!--   class_stats <- cbind(CM$byClass, prob_stats) -->
<!--   class_stats <- colMeans(class_stats) -->

<!--   #Aggregate overall stats -->
<!--   overall_stats <- c(CM$overall) -->

<!--   #Combine overall with class-wise stats and remove some stats we don't want  -->
<!--   stats <- c(overall_stats, class_stats) -->
<!--   stats <- stats[! names(stats) %in% c('AccuracyNull',  -->
<!--     'Prevalence', 'Detection Prevalence')] -->

<!--   #Clean names and return -->
<!--   names(stats) <- gsub('[[:blank:]]+', '_', names(stats)) -->
<!--   return(stats) -->

<!-- }) -->

<!-- #Fit model -->
<!-- set.seed(19556) -->

<!-- # test <- within(data, Contraceptive_methodClass <- ifelse(data$Contraceptive_method==1, 'R1', ifelse(data$Contraceptive_method==2, 'R2', 'R3'))) -->
<!-- # test <- subset(test, select = -c(Contraceptive_method)) -->
<!-- # test$Contraceptive_methodClass <- factor(test$Contraceptive_methodClass) -->
<!-- # levels(test$Contraceptive_methodClass) -->

<!-- levels(test$Contraceptive_method) -->
<!-- model <- train(formula=Contraceptive_method ~ . , data = test, method='knn', tuneGrid=expand.grid(.k=1:30), metric='Accuracy', -->
<!--   trControl=trainControl(method='repeatedcv', number=10, repeats=15, classProbs=TRUE, summaryFunction=multiClassSummary)) -->

<!-- #Stop parallel cluster -->
<!-- # stopCluster(cl) -->

<!-- for(stat in c('Accuracy', 'Kappa', 'AccuracyLower', 'AccuracyUpper', 'AccuracyPValue',  -->
<!--               'Sensitivity', 'Specificity', 'Pos_Pred_Value',  -->
<!--               'Neg_Pred_Value', 'Detection_Rate', 'ROC', 'logLoss')) { -->

<!--   plot(model, metric=stat) -->
<!-- } -->
<!-- ``` -->
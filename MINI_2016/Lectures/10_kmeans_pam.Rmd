---
title: "Combinatorial and Model Based Clustering"
author: "Przemyslaw Biecek"
date: "Data Mining - Advances"
output: 
  html_document:
    toc: TRUE
---

Last time we have discussed hierarchical clustering. Today we will introduce two combinatorial methods and one model based method.

Combinatorial Clustering Methods

* k-means
* Partitioning Around Medoids (PAM)

Model Based Clustering Methods

* mclust (mixture of Gaussian distributions)

In both cases, one specifies the initial configuration and the number of clusters. Then in iterative fashion the optimal clustering is obtained. 

In case of combinatorial clustering the clusters are defined by medoids. Observations are assigned to clusters defined by the closest medoid. 
The difference between kmeans and PAM method come from the fact that in the PAM method the medoids are observations/rows in the dataset while for k-means any point may be a medoid.

In case of model based clustering clusters are defined by means and variance-covariance matrices. Observations are assigned to clusters defined by loglikelihoods.

# The dataset

Let's use the dataset `Cars93` as an example. The dataset contains information about length and weight of different cars. 

```{r, fig.width=10, fig.height=10}
library(ggplot2)
library(MASS)
library(cluster)

ggplot(Cars93, aes(Length, Weight, label=Make)) +
  geom_text(size=3) + 
  theme_bw()
```

# k-means

The algorithm is following:

1. Initialize with random medoids
2. Assign observations to closest medoids
3. Calculate canters for each cluster, these are new medoids, 
4. Go to 1 (repeat until convergence).

## The R code

```{r}
Cars93$Length <- scale(Cars93$Length)
Cars93$Weight <- scale(Cars93$Weight)

set.seed(4)

model1 <- kmeans(Cars93[,c("Length","Weight")], 10)
Cars93$cluster <- factor(model1$cluster)
nd <- data.frame(model1$centers)
ggplot(Cars93, aes(Length, Weight)) +
  geom_text(size=3, aes(label=Make, color=cluster)) + 
  geom_point(data=nd, size=5)+
  theme_bw()

# Second attempt
model2 <- kmeans(Cars93[,c("Length","Weight")], 10)
Cars93$cluster <- factor(model2$cluster)
nd <- data.frame(model2$centers)
ggplot(Cars93, aes(Length, Weight)) +
  geom_text(size=3, aes(label=Make, color=cluster)) + 
  geom_point(data=nd, size=5)+
  theme_bw()
```

! Note that this algorithm is unstable. Run the code above few times to see different results.

## Your turn

The total sum of squares is decomposed into within sum of squares (`withinss`) and between sum of squares (`betweenss`). 

Check structure of results generated with kmeans function and find slots with both components. In the loop repeat the procedure `keamns()` 10 times and present the possible values of `betweenss`. 

Are results from different runs different? Which to choose?

```{r}
model1$totss
model1$betweenss
model1$withinss

model2$betweenss
```

# PAM

The algorithm is following:

1. Initialize with random medoids
2. Assign observations to closest medoids
3. Swap medoid with other observation if this this will improve the fit 
4. Go to 1 (repeat until convergence).

## The R code

```{r}
model4 <- pam(Cars93[,c("Length","Weight")], 10)
Cars93$cluster <- factor(model4$clustering)

nd <- data.frame(model4$medoids)

ggplot(Cars93, aes(Length, Weight)) +
  geom_text(size=3, aes(label=Make, color=cluster)) + 
  geom_point(data=nd, size=5)+
  theme_bw()
```

## Silhouettes

How to choose the number of clusters?

The popular indexes that helps to choose number of clusters are `silhouette`'s. They are defined as

$$
s(i) = (b(i) - a(i))/max(a(i), b(i))
$$

where $a(i)$ is the average distance to all observations from same cluster while $b(i)$ is the average distance to all observations from the second closest cluster (called `neighbor`).

Note that this value can be negative!

```{r}
dissE <- daisy(as.matrix(Cars93[,c("Length","Weight")])) 
si1 <- silhouette(model1$cl, dissE)
plot(si1, main="k-means")

si2 <- silhouette(model4)
plot(si2,main="PAM")

mylist <- list(
  pam(Cars93[,c("Length","Weight")], 5),
  pam(Cars93[,c("Length","Weight")], 6),
  pam(Cars93[,c("Length","Weight")], 7),
  pam(Cars93[,c("Length","Weight")], 8),
  pam(Cars93[,c("Length","Weight")], 9),
  pam(Cars93[,c("Length","Weight")], 10)
)

sapply(mylist, function(x) mean(silhouette(x)[,3]))

```

The average silhouette could be used as a good index of clustering

```{r}
head(si2)
mean(si2[,3])
str(si2)
```

## Your turn

For the `Cars93` dataset find the *optimal* number of clusters based on average silhouette.

```{r, eval=FALSE}
model4$silinfo
```

## Note

In the `kmeans` function there is a parameter `nstart` - number of random starts. The default is 1 but you may increase it in order to get more stable results.

All issues related to hierarchical clustering still holds (like the variable normalization).

# Model based - mclust

In this clustering we assume that data is generated from mixture of Gaussian distributions. Our goal is to identify parameters of this mixture.

The algorithm is following

1. Define a set of model structures (see `?mclustModelNames`), structures are related to restrictions over parameters in vectors of means and variance-covariance matrices,
2. For each model structure use EM algorithm to fit model parameters to the data,
3. Choose the model with best BIC score

In general the BIC criteria is defined as

$$
BIC(M) = -2 * loglikelihood(M) + npar(M) * log(n)
$$

where $n$ is the number of observations, $npar(M)$ is the number of parameters in the model $M$ while $loglikelihood(M)$ it the model log likelihood. 

## The R code

We are going to use data about `Old Faithful Geyser`.

```{r, warning=FALSE, message=FALSE}
library(mclust)
plot(faithful)
```

Let's see now some examples for 3 and 2 clusters and different structures.

```{r, warning=FALSE, message=FALSE}
mod3 = Mclust(faithful)
summary(mod3, parameter = TRUE)
plot(mod3, what = "BIC")
plot(mod3, what = "classification")
plot(mod3, what = "uncertainty")
plot(mod3, what = "density")

mod3 = Mclust(faithful, G=2)
summary(mod3, parameter = TRUE)
plot(mod3, what = "classification")
```

Instead of fitting all possible structures you can be more specific. 

```{r, warning=FALSE, message=FALSE}
mod3 = Mclust(faithful, G=2, modelNames="VEI")
plot(mod3, what = "classification")
mod3 = Mclust(faithful, G=2, modelNames="EVE")
plot(mod3, what = "classification")
```

You can compare different models with the use of BIC criteria

```{r, warning=FALSE, message=FALSE}
mod3 = Mclust(faithful, G=2:5, modelNames=c("VVV","VEI","EVE"))
mod3$BIC
```

## Your turn

For the `faithful` dataset try different number of clusters from 2 to 20 and plot the BIC scores for different number of clusters.

# Computer classes

Use data from voting’s of deputies from Polish Sejm (previous cadence).

The task: 
Cluster voting profiles for different deputies and present them graphically.

Compare PAM vs kmeans vs hclust approaches.

```{r}
load("all_votes.rda")
head(all_votes[,1:7])
```

Details:

- (as during last classess) create a matrix with deputies in rows and id_voting in columns (you can use `spread()` function from `tidyr`),
- choose subset of voting’s,
- cluster deputies.

If you cannot create such matrix for all deputies, choose only deputies that were present during majority of votings.

With the use of `all_votes` data:

* for both PAM and k-means find clustering for different number of clusters (from 2 to 20). * for each observation calculate the `silhouette` and then calculate the average silhouette score. 
* plot the average silhouette as a function of number of clusters.

Create a 'one-page' cheat-sheet for clustering. Select most important function and summaries how to use it and why to use it at all.

As an inspiration you may use:
https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf
https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf




---
title: |
    | Poject #1 - Classification 
    | Class in social networks
author: "Viet Ba Mai, Klaudia Magda"
date: "November 6th, 2016"
output: 
  html_document:
    toc : TRUE
---

#Introduction
The goal of this project is to predict the number of shares in social networks using the *Online News Popularity* dataset from UCI.
In this report we will implement several classifiers on preprocessed set using variables selected with the help of filters and compare their performance in order to find the best one.


#Libraries used
```{r, message=FALSE, warning=FALSE}
library(e1071)
library(caret)
library(magrittr)
library(plyr)
library(dplyr)
library(corrplot)
library(randomForest)
library(pROC)
library(Epi)
library(nnet)
library(party)
library(rpart)

```


#Data preparation
##Target variable
The target variable of the set is `shares`. The first thing one can notice is that it contains almost 1500 distinct values, where more of them occur only once. This means that this variable has a very high variance and trying to train the classifiers with it would yield very inaccurate result. More than that the feature selection function we will use in the next sections are unable to handle such a big number of classes.

Our solution to this problem was to create a new target variable with only a few classes representing the ranges of shares.

```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
data <- read.csv(file.path(file.path(getwd(), "OnlineNewsPopularity"), "OnlineNewsPopularity.csv"), header=TRUE, sep=",", encoding="UTF-8")
sum(!complete.cases(data))
#Removing rows with NaN
data <- data[complete.cases(data),]
plot(density(data$shares), xlim=c(0,10000), col="green")
categories = cut(data$shares, c(0, 1000, 2000, Inf))
data$Class = as.numeric(categories)
data$Class = factor(data$Class)
length(levels(data$Class))
par(mar = rep(2, 4))
barplot(table(data$Class), col="lightgreen", main="Distribution of Class variable")
prop.table(table(data$Class))
```


##Preprocessing
In order to make working with the data easier we immediatelly dropped the original target variable `shares` as well as non-predictive columns `url`, `timedelta` and `shares` and then removed near zero variance and highly correlated variables.
```{r, echo=TRUE, message=FALSE, warning=FALSE}
data <- subset(data, select = -c(url, timedelta, shares))

#Removing near zero variance variables
nzv <- nearZeroVar(data)
names(data)[nzv]
#Normalisation
preProcValues <- preProcess(data, method = c("range"))
dataNorm <- predict(preProcValues, data)
#Removing highly correlated variables
d.num <- dataNorm %>% select(which(sapply(dataNorm, is.numeric)))
too_high <- findCorrelation(cor(d.num), cutoff = 0.725, verbose = FALSE)
names(d.num)[too_high]
data = dataNorm[,-c(too_high)]
```


#Feature selection
After data preparation we were still left with over 40 predictors.
In order to choose only the best ones for classification we decided to use a `filter` method - Selection by filter (`sbf`) on a smaller sample of the dataset.
```{r, echo=TRUE, message=FALSE, warning=FALSE}
size <- floor(0.05 * nrow(data))
indxSample <- sample(seq_len(nrow(data)), size = size)
sample <- data[indxSample, ]
length(levels(sample$Class))
#SBF
filterCtrl <- sbfControl(functions = rfSBF, method = "repeatedcv", verbose = FALSE, repeats = 5)
rfWithFilter <- sbf(form = Class ~ ., data = sample, sbfControl = filterCtrl, allowParallel = TRUE, variables=TRUE)
rfWithFilter
selections <- rfWithFilter$variables$selectedVars
selections <- append(selections, "Class")
selections
```


###SBF result analysis
From the correlation plot we can see by the colour and the size of circles that the selected variables are not highly correlated which makes them good candidates for predictors.
```{r, echo=TRUE, message=FALSE, warning=FALSE}
subdata <- data[selections]
head(subdata)
summary(subdata)
sub.num <- subdata %>% select(which(sapply(subdata, is.numeric)))
corrplot(cor(sub.num), method="circle")
```


##Data Analysis

In this section 3 `pairs` plots and 3 scatter plots are presented to help us understand the given data. `Pairs` plot is a matrix of `scatter` plots which show relations between two variables. We decided to analyse 15 of them in 3 parts.

When the data has a line shape it means that the analysed data is of both continuous and categorical type. On the other hand when the shape is of singular dots in the corners it means that we are dealing with two categorical variables. Otherwise, when the markers are spread on a plane - two continuous features are analysed.

From the plots we can see that almost half of our features are discrete.
```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=12, fig.width=10}

 pairs(data[,1:5], main = "Variables 1-5", pch = "+", col = c("red", "green3",  "blue")[unclass(data$Class)])
 
 pairs(data[,6:10], main = "Variables 6-10",
 pch = "+", col = c("red", "green3",  "blue")[unclass(data$Class)])

pairs(data[,11:15], main = "Variables 11-15",
pch = "+", col = c("red", "green3",  "blue")[unclass(data$Class)])

xyplot(num_hrefs ~ num_imgs | Class, data, groups = data$Class, pch= 20)

xyplot(kw_min_min ~ kw_min_avg | Class, data, groups = data$Class, pch= 20)

xyplot(LDA_01 ~ LDA_02 |Class, data, groups = data$Class, pch= 20)

```


#Data splitting
Having the data prepared for classification we need to divide it into a training and testing set. We chose the size of the training set to be 0.75 of the original data set.
```{r, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(1313)
size <- floor(0.75 * nrow(subdata))
indxTrain <- sample(seq_len(nrow(subdata)), size = size)

dataTrain <- subdata[indxTrain, ]
dataTest <- subdata[-indxTrain, ]
```


#Classifiers

##Random Forest
Firstly we will use the random forest and then test whether using the top 10 predictors by importance given by the forest will yield better results.
Forest is a set of trees which contain a `vote` allowing us to find an appropriate result.
```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
forest <- randomForest(Class ~ ., data = dataTrain, importance = TRUE, na.action = na.omit)
varImpPlot(forest)
importance(forest)
predForest = predict(forest, dataTest, type="class")
plot(forest, main="Model Fit for Random Forest")
forestTab <- table(true = dataTest$Class, predicted = predForest)
rfAcc <- sum(diag(forestTab)) / sum(forestTab)
rfAcc
```


###Top variables by importance
```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
sortedImportance=order(-forest$importance[,5])
tops=rownames(forest$importance)[sortedImportance][1:15][1:15]
tops <- append(tops, "Class")
subTrain <- dataTrain[tops]
subTest <- dataTest[tops]
forestTop <- randomForest(Class ~ ., data = subTrain, na.action = na.omit)
predTopForest = predict(forestTop, subTest)
forestTopTab <- table(true = subTest$Class, predicted = predTopForest)
rfTopAcc <- sum(diag(forestTopTab)) / sum(forestTopTab)
rfTopAcc
```
The accuracy is very similar to the random forest working on more features hence for the remaining classifiers we will use only the 15 features selected by importance.


##Tree
Tree is undirected graph with nodes and leaf nodes that can indicate the value of Class.
```{r, echo=TRUE, message=FALSE, warning=FALSE}

tree<-ctree(Class ~ ., data=subTrain)
predTree = predict(tree, subTest)
treeTab <- table(true = subTest$Class, predicted = predTree)
treeAcc <- sum(diag(treeTab)) / sum(treeTab)
treeAcc
```


##Nearest Neighbours
This is a non-parametric method which input has `k` nearest examples from the training set in the space of numerical features.
```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
#Finding optimal k
tuneK <- 1:50
performance <- sapply(tuneK, function(k) {
  knnFit <- knn3(Class ~ ., data = subTrain, k=k)
  tab <- table(true = subTest$Class,
          predict = predict(knnFit, subTest, type="class"))
  sum(diag(tab)) / sum(tab)
}) 

optimal_k = which.max(performance)
optimal_k
performance[optimal_k]

df <- data.frame(tuneK, performance)

ggplot(df, aes(tuneK, performance)) +
  geom_point() + 
  geom_smooth(se=FALSE, span=0.1, size=2) +
  theme_bw()

###Applying optimal k
knnFit <- knn3(Class ~ ., data = subTrain, k=optimal_k)
predKnn <- predict(knnFit, subTest, type="class")
tab_optimal <- table(true = subTest$Class, predicted = predKnn)
knnAcc <- sum(diag(tab_optimal)) / sum(tab_optimal)
knnAcc
```


##Super Vector Machines
`SVM` classifier shows each data item on a n-dimensional plane, where `n` is the quantity of features (and their values being a particular coordinate) used and then performs classification to distinguish the classes on a plane.
It produces support vectors for each class.
We decided to use `SimonsSVM` library, because it allows options for using cells and threads number which decrease the execution time.
```{r, echo=TRUE, message=FALSE, warning=FALSE}
svm<-SimonsSVM::svm(Class ~ ., subTrain, useCells=TRUE, threads=3)
predSVM = predict(svm, subTest, type="class")
svmTab <- table(true = subTest$Class, predicted = predSVM)
svmAcc <- sum(diag(svmTab)) / sum(svmTab)
svmAcc
```


##Multinomial Logistic Regression
Logistic regression, also referred to as `logit model` is a predictive analysis.

Since we have more than two classes it is necessary to use the `multinomial logistic regression` rather than the basic binomial one. We will use the `multinom()` function from `nnet` library.
```{r, echo=TRUE, message=FALSE, warning=FALSE}
vglm <- multinom(Class ~ ., data=subTrain)
predVglm = predict(vglm, subTest, type="class")
vglmTab <- table(true = subTest$Class, predicted = predVglm)
vglmAcc <- sum(diag(vglmTab)) / sum(vglmTab)
vglmAcc
```


##Naive Bayes
`Naive Bayes` is a classifier that uses prior probability, to make a prediction.
It is called 'naive', because it simplifies the calculation of probabilities by assuming that each attribute belonging to a given class value is independent.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
bayes <- naiveBayes(Class ~ ., data = subTrain)
predBayes = predict(bayes, subTest, type="class")
bayesTab <- table(true = subTest$Class, predicted = predBayes)
bayesAcc <- sum(diag(bayesTab)) / sum(bayesTab)
bayesAcc
```


#Performance
We will check the performance of used classifiers with `confusion matrix`, Area Under Curve (`AUC`) and `Macro Averaged Metrics`.


##Confusion Matrix
Confusion matrix is a table describing performance of classifiers. Besides, `confusionMatrix()` function also provides additional statistics.
```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
#Random Forest
rfConfusionMatrix <- confusionMatrix(predForest, subTest$Class)
rfConfusionMatrix

#Forest - Top variables by importance
forestTopConfusionMatrix <- confusionMatrix(predTopForest, subTest$Class)
forestTopConfusionMatrix

#Tree
treeConfusionMatrix <- confusionMatrix(predTree, subTest$Class)
treeConfusionMatrix

#Nearest Neighbours
knnConfusionMatrix <- confusionMatrix(predKnn, subTest$Class)
knnConfusionMatrix

#SVM
svmConfusionMatrix <- confusionMatrix(predSVM, subTest$Class)
svmConfusionMatrix

#Multinomial Logistic Regression
mlrConfusionMatrix <- confusionMatrix(predVglm, subTest$Class)
mlrConfusionMatrix

#Naive Bayes
bayesConfusionMatrix <- confusionMatrix(predBayes, subTest$Class)
bayesConfusionMatrix


```


##Area Under Curve
Receiver Operating Characteristics curve (`ROC`) visualises the performance of a classifier. `AUC` is the Area Under the ROC Curve, where the value equal to 1 means a perfect test.
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
#Random Forest
rfAUC <- auc(predForest, as.numeric(subTest$Class))
rfrocobj <- plot.roc(predForest, as.numeric(subTest$Class))
plot(rfrocobj, col="blue")
par(new = TRUE)

#Top Variables
forestTopAUC <- auc(predTopForest, as.numeric(subTest$Class))
forestToprocobj <- plot.roc(predTopForest, as.numeric(subTest$Class))
plot(forestToprocobj, col="black", add=TRUE)
par(new = TRUE)

#Tree
treeAUC <- auc(predTree, as.numeric(subTest$Class))
treerocobj <- plot.roc(predTree, as.numeric(subTest$Class))
plot(treerocobj, col="yellow", add=TRUE)
par(new = TRUE)

#Nearest Neighbours
knnAUC <- auc(predForest, as.numeric(subTest$Class))
knnrocobj <- plot.roc(predKnn, as.numeric(subTest$Class))
plot(knnrocobj, col="green", add=TRUE)
par(new = TRUE)

#SVM
svmAUC <- auc(predSVM, as.numeric(subTest$Class))
svmrocobj <- plot.roc(predSVM, as.numeric(subTest$Class))
plot(svmrocobj, col="red", add=TRUE)
par(new = TRUE)

#Multinomial Logistic Regression
vglmAUC <- auc(predVglm, as.numeric(subTest$Class))
vglmrocobj <- plot.roc(predVglm, as.numeric(subTest$Class))
plot(vglmrocobj, col="orange", add=TRUE)
par(new = TRUE)


#Naive Bayes
bayesAUC <- auc(predBayes, as.numeric(subTest$Class))
bayesrocobj <- plot.roc(predBayes, as.numeric(subTest$Class))
plot(bayesrocobj, col="pink", add=TRUE)


legend('bottomright', c('Random Forest', 'TopVariables' ,'K-NN', 'SVM', 'Log. Regression', 'Tree', 'Naive Bayes'), lty=1, col=c('blue', 'black', 'green', 'red',' orange', 'yellow', 'pink'), bty='n', cex=.75)
```




##Macro-Averaged Metrics
Tables for `In Macro-averaged Metrics` give an average of the following:

- `Precision = TP / (TP+FP)`, fraction of correct predictions for a certain class.

- `Recall = TP / (TP+FN)`, instances of a class that were correctly predicted. 

- `F1 = (Precision*Recall) / (Precision+Recall)`


Precision shows how consistent are those evaluations - how many values were correctly estimated.

F1 is a harmonic mean of precision and recall. That is why has a very similar values like Precision and Recall and shows that those values are 
not various from each other.
```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}

#RandomForest

forestPrec<-(diag(forestTab))/(colSums(forestTab))
forestRecall<-(diag(forestTab))/(rowSums(forestTab))
forestF1<-2*forestPrec*forestRecall/(forestPrec+forestRecall)

data.frame(forestPrec, forestRecall, forestF1) 

mForestPrec = mean(forestPrec)
mForestRecall = mean(forestRecall)
mForestF1 = mean(forestF1)
mamForest<-data.frame(mForestPrec, mForestRecall, mForestF1)
mamForest

#TopForest

forestTopPrec<-(diag(forestTopTab))/(colSums(forestTopTab))
forestTopRecall<-(diag(forestTopTab))/(rowSums(forestTopTab))
forestTopF1<-2*forestTopPrec*forestTopRecall/(forestTopPrec+forestTopRecall)

data.frame(forestTopPrec, forestTopRecall, forestTopF1) 

mForestTopPrec = mean(forestTopPrec)
mForestTopRecall = mean(forestTopRecall)
mForestTopF1 = mean(forestTopF1)
mamForestTop<-data.frame(mForestTopPrec, mForestTopRecall, mForestTopF1)
mamForestTop

#Tree

treePrec<-(diag(treeTab))/(colSums(treeTab))
treeRecall<-(diag(treeTab))/(rowSums(treeTab))
treeF1<-2*treePrec*treeRecall/(treePrec+treeRecall)

data.frame(treePrec, treeRecall, treeF1) 

mTreePrec = mean(treePrec)
mTreeRecall = mean(treeRecall)
mTreeF1 = mean(treeF1)
mamTree<-data.frame(mTreePrec, mTreeRecall, mTreeF1)
mamTree

#Nearest Neighbours

knnPrec<-(diag(tab_optimal))/(colSums(tab_optimal))
knnRecall<-(diag(tab_optimal))/(rowSums(tab_optimal))
knnF1<-2*knnPrec*knnRecall/(knnPrec+knnRecall)

data.frame(knnPrec, knnRecall, knnF1) 

mKnnPrec = mean(knnPrec)
mKnnRecall = mean(knnRecall)
mKnnF1 = mean(knnF1)
mamKnn<-data.frame(mKnnPrec, mKnnRecall, mKnnF1)
mamKnn

#SVM

svmPrec<-(diag(svmTab))/(colSums(svmTab))
svmRecall<-(diag(svmTab))/(rowSums(svmTab))
svmF1<-2*svmPrec*svmRecall/(svmPrec+svmRecall)

data.frame(svmPrec, svmRecall, svmF1) 

mSvmPrec = mean(svmPrec)
mSvmRecall = mean(svmRecall)
mSvmF1 = mean(svmF1)
mamSvm<-data.frame(mSvmPrec, mSvmRecall, mSvmF1)
mamSvm

#Multinomial Logistic Regression

vglmPrec<-(diag(vglmTab))/(colSums(vglmTab))
vglmRecall<-(diag(vglmTab))/(rowSums(vglmTab))
vglmF1<-2*vglmPrec*vglmRecall/(vglmPrec+vglmRecall)

data.frame(vglmPrec, vglmRecall, vglmF1) 

mVglmPrec = mean(vglmPrec)
mVglmRecall = mean(vglmRecall)
mVglmF1 = mean(vglmF1)
mamVglm<-data.frame(mVglmPrec, mVglmRecall, mVglmF1)
mamVglm

#Bayes

bayesPrec<-(diag(bayesTab))/(colSums(bayesTab))
bayesRecall<-(diag(bayesTab))/(rowSums(bayesTab))
bayesF1<-2*bayesPrec*bayesRecall/(bayesPrec+bayesRecall)

data.frame(bayesPrec, bayesRecall, bayesF1) 

mBayesPrec = mean(bayesPrec)
mBayesRecall = mean(bayesRecall)
mBayesF1 = mean(bayesF1)
mamBayes<-data.frame(mBayesPrec, mBayesRecall, mBayesF1)
mamBayes

```


#Conclusions
```{r, echo=FALSE, message=FALSE, warning=FALSE}
sum_table <- matrix(c(rfAcc, rfAUC, mForestPrec, mForestRecall, mForestF1, rfTopAcc, forestTopAUC, mForestTopPrec, mForestTopRecall, mForestTopF1, treeAcc, treeAUC, mTreePrec, mTreeRecall, mTreeF1, knnAcc, knnAUC, mKnnPrec, mKnnRecall, mKnnF1, svmAcc, svmAUC, mSvmPrec, mSvmRecall, mSvmF1, vglmAcc, vglmAUC,mVglmPrec, mVglmRecall, mVglmF1,  bayesAcc, bayesAUC, mBayesPrec, mBayesRecall, mBayesF1), ncol=5, nrow = 7 , byrow = TRUE)
colnames(sum_table) <- c("Accuracy", "AUC","Precision","Recall","F1")
rownames(sum_table) <- c("RF", "TOPRF", "TREE", "KNN", "SVM", "MLR", "BAYES")
sum_table
```

Our task was to find the best classification model for given data set and compare their performance with various methods hence we implemented 6 
distinct classifiers.
In the summary table above given are: `Accuracy`, `AUC`, `Precision`, `Recall` and `F1`.

Even though the results vary by few percent between the models, the one that yields the highest ones is the **random forest** - it has the best 
results for every method of performance, 48% of accuracy for all variables chosen with `sbf` and 46% for the top variables. Moreover, random 
Forest has the highest level of Precision, Recall and F1 (47%). `Area Under Curve` also confirms that random forest is the most effective methods 
in our project as it has the value closest to 1. 

We believe this is due to the fact that random forest uses multiple decision trees which makes it one of the most powerful classifiers and is 
thus more suitable for such complex data sets.

Nevertheless, other models are not that far from the random forest as their performance is on a similar level, `42-43%`.


To have a better view on what would be an expected accuracy of this task we checked the public leaderboard of `Online News Popularity` competition on Kaggle (ttps://inclass.kaggle.com/c/predicting-online-news-popularity/leaderboard/public ). It is written that `0.39130` is the `Performance you can expect to achieve with a simple application of a nearest neighbor algorithm without any optimization whatsoever` as well as `0.21739` being `The lowest possible performance you should be able to achieve, based on randomly sampling from a set of 5 classes` for a random classification.

In the first phase of the project we implemented a binary classification (for shares below 1400 or otherwise) with a higher result of `58%`.
On the other hand this phase shows a multiclass classification with 3, balanced classes. After some experiments we found that the less balanced distribution of classes, the lower results we obtain (dropping even below `40%`) hence our decision to split the shares as evenly as possible.

However, in both cases we obtained results higher than a simple chance, because for a binary it would be `~50%` and for 3 classes `~33%` is the chance to take a correct guess when making a random decision. This means that we surpassed it by over `10%`.


In conclusion, both the binary and multiclass classification experiments showed that Random Forest yields the highest results thus is is the best classifier we tested.

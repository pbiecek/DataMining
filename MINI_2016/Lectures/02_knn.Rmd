---
title: "k-nearest neighbors algorithm"
author: "Przemyslaw Biecek, Anna Wróblewska"
date: "Data Mining - Advances"
output: 
  html_document:
    toc: TRUE
---

# Introduction

In this part we are going to cover following topics:

* basics of k-nearest neighbors algorithm
* how to create knn classifier in R
* how to choose the k parameter in knn classifier

We are going to work with two datasets: `iris`. Iris dataset was originally created in 1935 by Anderson. This famous data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.

# Overview of the k-nearest neighbors method

The k-nearest neighbors algorithm is a distance based method. The default distance is the Euclidian one, but choose the metric wisely. 

Let's start with an example based on two variables / two dimensions. 
The example below is based on the `iris` data.

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
data("iris")

ggplot(iris, aes(Sepal.Length, Sepal.Width, color=Species)) +
  geom_point() +
  theme_bw() + coord_fixed()
```

The kNN algorithm works in three steps:

* For a new sample a set of k-nearest samples from the training dataset is identified. Let's denote this set as NN. 
* For samples in the set NN the distribution of classes is calculated. 
* For the new sample the final class is established by majority voting. Each sample in NN votes for it’s class.

```{r}
ggplot(iris, aes(Sepal.Length, Sepal.Width, color=Species)) +
  geom_point() +
  theme_bw() + coord_fixed() +
  geom_point(x=5.7, y=3.3, size=35, color="black", shape=1) +
  geom_point(x=5.7, y=3.3, size=2, color="black")
```

# How to train and use kNN in R

Let's train this classifier with R. 
The kNN method is implemented in a few different packages. Here we are going to use `knn3()` from `caret`. 

```{r, message=FALSE, warning=FALSE}
library(caret)
knnFit <- knn3(Species ~ Sepal.Length+Sepal.Width, data = iris, k=5, prob=TRUE)
knnFit
```

The trained classifier is in the `knnFit` object. 
Use the `predict()` function  in order to apply it to a new data set.
By default it returns distribution of votes for each sample. Use `type="class"` to predict the class of new samples.

```{r}
pred <- predict(knnFit, data.frame(Sepal.Length = 5.7, Sepal.Width= 3.6))
t(pred)
par(mar=c(5,3,3,3))
barplot(pred, las=2)

# majority voting
predict(knnFit, data.frame(Sepal.Length = 5.7, Sepal.Width= 3.6), type="class")
```

# Quality of the classifier

How to asses how good is the trained classifier?

Let's start with a contingency table for the predicted labels crossed with true labels.

```{r}
knnFit <- knn3(Species ~ Sepal.Length+Sepal.Width, data = iris, k=1)
pred <- predict(knnFit, iris, type="class")

tab <- table(true = iris$Species, predicted = pred)
tab

sum(diag(tab)) / sum(tab)
```

## Over fitting

When it looks like the performance is 100%. Is it possible? Maybe, but here it is a result of over fitting.

Note: One should not calculate performance of a classifier on the same dataset that have been used for training. Never!

What else can we do?

## Testing and Training

Let's divide the dataset into two subsets: training and testing.

```{r}
set.seed(1313)
indxTrain <- createDataPartition(y = iris$Species, p = 0.75)
str(indxTrain)

irisTrain <- iris[indxTrain$Resample1,]
irisTest <- iris[-indxTrain$Resample1,]
```

Now we can train classifier on the training dataset and test it on the second dataset.

```{r}
knnFit <- knn3(Species ~ Sepal.Length+Sepal.Width, data = irisTrain, k=1)
pred <- predict(knnFit, irisTest, type="class")

tab <- table(true = irisTest$Species, predicted = pred)
tab
tab2 <- prop.table(tab, 1)
tab2

sum(diag(tab)) / sum(tab)
sum(diag(tab2)) / sum(tab2)
```

# How to choose k?

The performance may be assessed for different values of parameter k. Based on such results one can select the ,,optimal'' k, i.e. k that maximizes some measure of performance. Here the performance is just the fraction of correct guesses (called accuracy). We will introduce more measures for performance later.

```{r}
tuneK <- 1:100
performance <- sapply(tuneK, function(k) {
  knnFit <- knn3(Species ~ Sepal.Length+Sepal.Width, data = irisTrain, k=k)
  tab <- table(true = irisTest$Species,
          predict = predict(knnFit, irisTest, type="class"))
  sum(diag(tab)) / sum(tab)
}) 

df <- data.frame(tuneK, performance)

ggplot(df, aes(tuneK, performance)) +
  geom_point() + 
  geom_smooth(se=FALSE, span=0.1, size=2) +
  theme_bw()

```

# Other issues

Note that by default the Euclidian distance is used. To make sure that all variables have the same impact on results one should first normalize each variable that is used for predictions. It is not obligatory, but it is a good practice if different variables are measured in different units.

Note that majority voting is not best solution if classes have unequal distribution. If one class is predominant then it may happen that the classifier will vote for it in every case. In order to deal with unbalanced labels one can consider weighted votes.

Look at this plot below. Iris data have different scales and they are not linear separable. What about normalization?
```{r}
summary(iris[,c("Sepal.Length","Sepal.Width")])

library(plotly)
library(magrittr)
plot_ly(iris, x = ~Sepal.Width, y = ~Petal.Width, z = ~Petal.Length, color=~Species) %>% add_markers()
```

# Computer classes

1. Train the kNN classifier for the `iris` dataset (for 3 or 4 variables). Use different implementations of kNN (packages: `class`, `kknn`). 

2. Calculate the performance for the classifiers and compare the results.

3. Normalize all predictive variables (Length's and Width's).

4. Calculate the performance for classifier build over normalized variables (compare with point 2).

# Home work

Use `knitr` to create a report for kNN method based on `student-mat.csv` dataset.
Choose variables and build classifier for them. Try use the response/target variables  'Dalc' or 'Walc'. You need to make factors from these variables to use them as target viriables. Find optimal `k` and calculate performance for it.

# Additional materials

A Short Introduction to the `caret` package
https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf

https://www3.nd.edu/~steve/computing_with_data/17_Refining_kNN/refining_knn.html

https://rstudio-pubs-static.s3.amazonaws.com/123438_3b9052ed40ec4cd2854b72d1aa154df9.html



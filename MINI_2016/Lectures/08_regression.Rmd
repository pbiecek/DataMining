---
title: "Regression"
author: "Anna Wróblewska"
date: "Data Mining - Advances"
output: 
  html_document:
    toc: TRUE
---

# Regression analysis

Regression analysis is used to describe the relationship between:

* A single response variable: $Y$ and
* One or more predictor variables: $X_1$, $X_2$, . . . , $X_p$
    - when $p = 1$ is Simple Regression
    - when $p > 1$ is Multivariate Regression

The response variable $Y$ must be a continuous variable.
The predictors $X_1$, $X_2$, . . . , $X_p$ can be continuous, discrete or categorical variables.
Linear regression objective is to generalize the simple regression methodology in order to describe the relationship between a response variable $Y$ and a set of predictors $X_1$, $X_2$, . . . , $X_p$ called exploratory variables.

So we have sets of observations: $(x_{11},...,x_{1p},y_1),...,(x_{n1},...,x_{np},y_n)$

Multivariate linear regression model is given by:
$y_i =\beta_0 +\beta_1*x_{i1} +\beta_2*x_{i2} +...+\beta_p*x_{ip} +ε_i$ for $i =1,...,n$ where:

* random error is $ε_i ∼ N(0, σ^2)$
* independent linear function is $\beta_1*x_{1} +\beta_2*x_{2}+ ... +\beta_p*x_{p}=E(Y|x_1,...x_p)$

Unknown parameters:

* $\beta_0$ is overall mean
* $\beta_k$, $k = 1,...,p$ are regression coefficients

##Estimation of regression line

As in the case of simple linear regression, we want to find the equation of the line that “best” fits the data. In this case, it means finding $b_0, b_1, ... , b_p$ such that the fitted values of $y_i$, given by

$\hat{y}_i =b_0+b_1*x_{1i} +...+ b_p*x_{pi}$, 

are as “close” as possible to the observed values $y_i$.

The difference between the observed value $y_i$ and the fitted value $\hat{y}_i$ is called *residual* and is given by:

$e_i = y_i − \hat{y}_i$

A way of calculating $b_0, b_1, ... , b_p$ is based on the minimization of the sum of the squared residuals, or residual sum of squares $RSS$ (Least Squares Method):

$RSS  =  \sum_i{e_i^2} =  \sum_i{(y_i−\hat{y}_i)^2} = \sum_i{(y_i −b_0 −b_1*x_{1i} −...− b_p*x_{pi})^2}$

The parameters $b_0, b_1, ... , b_p$ are estimated by using the function `lm()`.

#Some tips on exploratory data analysis                       
Once again we use pima dataset on diabetes in Pima Indian women.
```{r}
library(faraway)
pima$test <- factor(pima$test)
levels(pima$test) <- c("neg", "pos")
summary(pima)
hist(pima$glucose)
```
Note that categorical data should not be numerical, e.g. "test" values.

Variables “glucose”, “diastolic”, “triceps”, “insulin” and “bmi” have minimum value equal to zero. Is it possible to have glucose at zero value level? It seems that zero was used to code missing data. It should not be done because it can cause misleading results because for some variables it can be a valid value.

Set the missing values coded as zero to NA.
```{r}
pima$glucose[pima$glucose==0] <- NA
pima$diastolic[pima$diastolic==0] <- NA
pima$triceps[pima$triceps==0] <- NA
pima$insulin[pima$insulin==0] <- NA
pima$bmi[pima$bmi==0] <- NA

summary(pima) #summary with number of NA's
```

At first it is worth to do some graphical exploratory analysis one and more variables.
The scatter plot allows one to obtain an overview of the relations between variables.
```{r}
hist(pima$glucose)
plot(density(pima$glucose,na.rm=TRUE))
plot(triceps~bmi, pima)
boxplot(diabetes~test ,pima)
plot(pima, gap=0)
```
Now try a regression. Form the `pairs` plot we can see that there some correlated variables.
Fit the regression model using the function `lm()` and use a function `summary()` to get some results.
```{r}
pima.lm <- lm(glucose~insulin, data= pima)
summary(pima.lm, corr=TRUE)
```
`Intercept` is a free coefficient $\beta_0$. 

#Regression excersices
Now we will use an excersice from ISRL book. When you write `?Boston` in R console, you will see a decsription of the dataset.
```{r}
library(MASS)
data(Boston)
head(Boston)
```
Please do some exploratory analysis of this dataset: summaries, plots, e.g. the range of `medv` column and its quantiles.  
We will seek to predict `medv` (median house value) using 13 predictors such as `rm` (average number of rooms per house), `age` (average age of houses), and `lstat` (percent of households with low socioeconomic status).
```{r}
lm.fit=lm(medv~lstat ,data=Boston)  
#Short information
lm.fit
summary(lm.fit)
```
As you can see `summary` will give us an information about p- values and standard errors for the coefficients, as well as the R2 statistic and F-statistic for the model.  
In a simple linear regression setting, in order to determine whether there is a relationship between the response and the predictor we can simply check whether $\beta_1 = 0$. In the multiple regression setting with p predictors, we need to ask whether all of the regression coefficients are zero, i.e. whether $\beta_1 = \beta_2 = ··· = \beta_p = 0$. As in the simple linear regression setting, we use a hypothesis test to answer this question. We test the null hypothesis,  
$H_0 : \beta_1 = \beta_2 = ··· = \beta_p = 0$ (There is no relationship between variables)  
versus the alternative (There is any relationship)
$H_a : at least one \beta_j is non-zero$.
This hypothesis test is performed by computing the F-statistic and p-value.

If you want to see confidence intervals for regression parameters, just write:
```{r}
confint.lm(lm.fit)
```
Now we will try some predictions:
```{r}
predict(lm.fit,data.frame(lstat=c(5,10,15)), interval ="confidence")
predict(lm.fit,data.frame(lstat=c(5,10,15)), interval ="prediction")
```
For instance, the 95 % confidence interval associated with a lstat value of 10 is (24.47, 25.63), and the 95 % prediction interval is (12.828, 37.28). 
```{r}
plot(Boston$lstat ,Boston$medv) 
#or use attach(Boston) and then use only names of variables plot(lstat, medv)

abline(lm.fit,lwd=3,col="red")
par(mfrow=c(2,2))
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot(hatvalues(lm.fit))
which.max(hatvalues (lm.fit))
```
```{r}
lm.fit=lm(medv~lstat+age,data=Boston)
summary(lm.fit)
```
You can use also formluas such as
`medv~.` for all variables in the dataset, `medv~.-age` for all but one variables or even you can do non linear fitting with formulas: `medv ∼ lstat * age`, `medv∼lstat+I(lstat^2)`, `medv ∼ poly(lstat,5)`, `medv~log(rm)`.
```{r}
lm.fit=lm(medv~lstat,data=Boston)
lm.fit2=lm(medv~lstat+I(lstat^2), data=Boston)
anova(lm.fit ,lm.fit2)
```
The `anova()` function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the F-statistic is 135 and the associated p-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat2` is far superior to the model that only contains the predictor `lstat`. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`.

```{r}
par(mfrow=c(2,2))
plot(lm.fit2)
```


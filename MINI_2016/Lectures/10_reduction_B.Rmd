---
title: "Association rules and scaling"
author: "Przemyslaw Biecek"
date: "Data Mining - Advances"
output: 
  html_document:
    toc: TRUE
---

# Association rules

```{r, message=FALSE, warning=FALSE}
library(dplyr)
```

## Introduction

Let Z be the set of items and X, Y be a subsets of Z, called itemsets. 

A rule has form X -> Y.

We will use three measures that summarise a rule or it's itemsets.

`Support` of an itemset X is the percentage of transactions covered by X. If n is the number of all transactions and n_X transactions in X then

`supp(X) = n_X/n`

`Confidence` of a rule is the percentage of transactions with Y in all transactions covred by X.

`conf(X -> Y)` = `supp(X+Y) / supp(X)`


`Lift` is a measure of deviation from statistical independence. It takes value 1 if X and Y are independent and values greater than 1 for positive dependence.

`lift(X -> Y)` = `conf(X -> Y) / supp(Y)` = `supp(X+Y)/(supp(X) supp(Y))`


A good rule should has high support and high confidence. Usually we apply some filters for `supp(X+Y)` and `cond(X -> Y)`.

The procedure how to find frequent itemsets and good rules is non trivial. See for example a very good description http://michael.hahsler.net/research/arules_RUG_2015/demo/ and http://michael.hahsler.net/research/arules_RUG_2015/talk/arules_RUG2015.pdf

## Basic Example

We will use `apriori` function from `arules` package to find rules based on `Groceries` dataset. 

Let's see how the data is stored (sparse matrix).

```{r, message=FALSE, warning=FALSE}
library("arules")
data("Groceries")
Groceries

head(Groceries@itemInfo)
head(Groceries@data,5)

image(head(Groceries,300))
```

Now we can extract some rules.

```{r, message=FALSE, warning=FALSE}
rules <- apriori(Groceries, parameter = list(support = .001))

rules
inspect(head(sort(rules, by = "lift"), 3))
```

Your turn: change parameters to have rules with confidence > 0.9.

## A bit more complicated example

Let's try to apply this approach to our dataset.

We need to convert data about visited stations into the matrix with transactions.

```{r, message=FALSE, warning=FALSE}
#load("verySmallLogs.rda")

#trans <- verySmallLogs %>%
#  group_by(visitor) %>%
#  summarise(path = paste(sort(unique(substr(grep(station, #pattern="^[^1]", value=TRUE), 1, 5))), collapse=";"))

#save(trans, file="trans.rda")
load("trans.rda")
head(trans,10)
```

For each visitor we have stations that have been visited. Let's convert this data into a list of vectors. This format is supported by the `apriori` function.

```{r, message=FALSE, warning=FALSE}
listTrans <- strsplit(as.character(as.data.frame(trans)[,2]), split=";")
names(listTrans) <- paste0("U",as.data.frame(trans)[,1])
head(listTrans)

# convert into transactions
trans1 <- as(listTrans, "transactions")
summary(trans1)
image(head(trans1,20))
```

Now we are ready to mine rules in such dataset. In order to speed up the process we can add additional restrictions for maximum size of itemsets.

```{r, message=FALSE, warning=FALSE}
rules <- apriori(trans1, parameter = list(support = .001, maxlen=10, confidence=0.7))

rules
inspect(head(sort(rules, by = "lift"), 10))
inspect(head(sort(rules, by = "support"), 10))
```

Our rules have very small support.

We can train them on much larger dataset to get larger support.

# Multidimensional scaling

Working with high dimensional data we are unable to properly visualise the dataset or sometimes our methods are not capable to deal with too high dimension. 

So we may use some techniques to reduce the dimension in our data

## Principal Components Analysis

### Introduction

The most basic technique is PCA.
It' goal is to find another parameterisation of the whole space in which variance in the data is highest in consecutive dimensions.
Then usually only first two or three dimensions in the new dataset is left.

Important: New variables are orthogonal and are linear transformations of the original variables. 

In R it's implemented in two function `princomp` (uses the eigen value decomposition) and `prcomp` (uses SVD decomposition).

![](pca.png)

### Basic example

Let us start with an example

```{r}
head(USArrests)
pca <- princomp(~ Murder + Assault + UrbanPop,
                  data = USArrests, cor = TRUE)
summary(pca)
pca$scores[1:5, ]
plot(pca)
biplot(pca)
pca$loadings
```

### A bit more complicated

How we can apply this to larger dataset?

```{r}
library(tidyr)
nvisits <- verySmallLogs %>%
  group_by(visitor, station) %>%
  summarise(count = n()) %>%
  spread(station, count)

head(nvisits)
nvisits[is.na(nvisits)] <- 0
head(nvisits)

pcaL <- princomp(nvisits[-1,-1])
summary(pcaL)
pcaL$scores[1:5,]
plot(pcaL)

pcaL$loadings



clusters <- kmeans(nvisits[-1,-1], 50)
onlycenters <- clusters$centers
ndf <- sammon(dist(onlycenters), k = 2)
plot(ndf$points, col="white")
text(ndf$points[,1], ndf$points[,2], 1:15)
```

## Isometric scaling

PCA is very popular, but it is not the only method that can be used for dimension reduction. Other, very popular one, is the Kruskal's non-metric multidimensional scaling. 

There are few interesting and important differences.

* As an input we will use distance matrix not original data. Thus we can use this method even if we are working with non numerical data (e.g. documents). 
All we need is to define dissimilarity
* Solution is calculated through numerical optimization, sometimes it can be unstable.
* The new coordinated are derived in such a way, that the overall stress is minimized, where stress is defined as the sum of squared differences between the original and transformed dataset (but other definitions of stress are also valid).


### Basic example

Let us start with an example

Sammon scaling - the sum of squared differences between the initial distances and obtained ones, weighted by the distances

```{r}
library(MASS)
head(USArrests)
distances <- dist(scale(USArrests))

USArrests <- unique(USArrests)
distances <- distances+0.01

mds1 <- sammon(distances, k=2)
str(mds1)

clusters <- kmeans(USArrests, 10)
mds1 <- sammon(dist(clusters$centers), k=2)
plot(mds1$points, pch=19, col="white")
text(mds1$points[,1], mds1$points[,2], 1:10)


plot(mds1$points, pch=19, col=clusters$cluster)
```

We can then see some diagnostics

```{r}
stress <- Shepard(dist(clusters$centers), mds1$points)
plot(stress)
```

Kruskal scaling - different objective, square root of the ratio of squared difference between distances divided by squares of initial distances.

```{r}
mds2 <- isoMDS(distances, k=2)
str(mds2)
plot(mds2$points)
```

We can then see some diagnostics

```{r}
stress <- Shepard(distances, mds2$points)
plot(stress)
```




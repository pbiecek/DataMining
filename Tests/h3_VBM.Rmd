---
title: "DMA - Homework 3"
author: "Viet Ba Mai"
date: "26 pazdziernika 2016"
output: 
  html_document:
  toc: TRUE
---

#Input data
`student-mat` is a .csv file with data regarding alcohol consumption among math course students.
10 first observations of selected columns from the imported table are presented below:
```{r, echo=FALSE, cache=TRUE}
student_mat <- read.csv(file="student-mat.csv", header=TRUE, sep=";", encoding="UTF-8")
head(student_mat[, c("Dalc", "studytime", "age", "absences", "failures", "freetime", "goout")], 10)
```
In this homework I will be creating `decision trees` and `random forests`.

#Decision Tree
In order to build a decision tree I will use the `party` library as it is better for visualisation than the traditional `rpart` library.
```{r, message=FALSE, warning=FALSE, cache=TRUE, fig.align='center'}
#install.packages("party")
library(party)
```
```{r, cache=TRUE}
student_mat$Dalc <- factor(student_mat$Dalc)
dalcTree <- ctree(Dalc ~ studytime + age + absences + failures + freetime + goout, data = student_mat)
print(dalcTree)
summary(dalcTree)
plot(dalcTree, main="Weekday Student's Alcohol Consumption Decision Tree")
```

We can observe that the variables `goout` and `studytime` were chosen as the ones that strongly affect the response with their p-values being very small - usually it is compared to a cutoff value `0.05` and if it's smaller then the `null hypothesis` is rejected which suggests that these values are statistically `significant`.

From the plot above we can see that students which go out with friends less often (less or equal to 3) have a much higher ratio of consuming alcohol only **one** day (class 1) during workdays to more than one (2-5).

In case when the `goout` value is higher than 3, the decision is dependent on the weekly hours of studying (`studytime`).

Students that study one or more hours a week also have a much higher ratio of 1 day od alcohol consumption than the remaining number of days.
Intuitively, the decision tree shows a reasonable result as it points out that the students that go out more often and study less have a more even distribution of  `Dalc` classes, so a similar number of students consume alcohol 3 times during workdays as 1 time.

#Random Forest
Forest is a set of trees which contain a `vote` allowing us to find an appropriate result. We will test it on different values of correlation between trees to find and optimal one by observing the OOB error. Different correlation can be achieved by selecting random input variables.

In order to do that I will make use of the `randomForest` library.
```{r, message=FALSE, warning=FALSE, cache=TRUE}
#install.packages("randomForest")
library(randomForest)
```

##Random forest on the same variables
Firstly we will try to train the forest on the same variables as in the previous section.
```{r, cache=TRUE, fig.align='center'}
ffit <- randomForest(Dalc ~ ., data=student_mat, importance = TRUE, prox=TRUE)
head(predict(ffit, type="prob"))
plot(ffit, main="Model fit on all variables")
varImpPlot(ffit, main="Importance Plot")
MDSplot(ffit, student_mat$Dalc, main="MDS Plot")
```
The `MDS (Multidimensional scaling) plot` is used to show the level of similarities between given cases. We can observe that the red, class #1 is widely spread and the remaining classes are clustered around the value of -0.2 of the 1st dimensions and the range of -0.1 to 0.1 for the second dimension.

Based on the importance plot and the variable importance we will select the variables for further testing.


##Mean Decrease in Accuracy
Mean Decrease in Accuracy is a measure of how much a model fit would decrease when a given variable is changed, or removed.
We will test 3 selected variables that have larger values of `MeanDecreaseAccuracy`.
```{r, cache=TRUE, fig.align='center'}
ffit2 <- randomForest(Dalc ~ G1 + sex + failures, data=student_mat, importance = TRUE)
head(predict(ffit2, type="prob"))
plot(ffit2, main="Model Fit for variables: G1, sex, failures")
```

##Mean Decrease in Gini
Gini is a measure of node purity that is used when calculating splits.
We will test 3 selected variables that have larger values of `MeanDecreaseGini`.
```{r, cache=TRUE, fig.align='center'}
ffit3 <- randomForest(Dalc ~ absences + goout + age, data=student_mat, importance = TRUE)
head(predict(ffit3, type="prob"))
plot(ffit3, main="Model Fit for variables: absences, goout, age")
```

##Randomly chosen variables
I decided to pick variables not used before in this section.
```{r, cache=TRUE, fig.align='center'}
ffit4 <- randomForest(Dalc ~ famsize + traveltime + activities, data=student_mat, importance = TRUE)
head(predict(ffit4, type="prob"))
plot(ffit4, main="Model Fit for variables: famsize, traveltime, activities")
```
#Performance comparison
```{r, cache=TRUE, fig.align='center'}
tab <- table(real = student_mat$Dalc, predicted = predict(dalcTree))
tab
sum(diag(tab)) / sum(tab)
print(ffit)
print(ffit2)
print(ffit3)
print(ffit4)
```
#Conclusions
The `OOB estimate of error rate` is very close for tested variable sets when using the forest tree. There is randomness included so each execution results in slightly different values for OOB, but they all were each time around `30%`, even for the randomly chosen variables. It is hard to distinguish the optimal `m` (variable set) for this case.

On the other hand the accuracy and performance of the decision tree indicates that its performance is only a little smaller than the on of the forest tree. For more complex problems using random forests may show a bigger difference in performance in comparison to decision trees as it uses multiple models and thus have a better performance than a single tree.
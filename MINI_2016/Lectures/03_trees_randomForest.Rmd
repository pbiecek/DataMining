---
title: "Decision trees and Random Forest"
author: "Przemyslaw Biecek"
date: "Data Mining - Advances"
output: 
  html_document:
    toc: TRUE
---

# Introduction

In this document we are going to cover following topics:

* A new dataset - Pima Indians
* Overview of classification trees
* Implementations in R
* Overview of Random Forest
* Implementations in R
* ROC 

## Pima Indians

Here we are going to use the `pima` dataset as a blasting site for classification trees.
The interesting outcome variable is `test` (1 if patient shows signs of diabetes, 0 means that the test for diabetes was negative). 
There are 8 interesting dependent variables, let’s start with two promising ones, namely glucose and insulin.

```{r, warning=FALSE, message=FALSE}
library(faraway)
library(MASS)
library(ggplot2)
head(pima)

ggplot(pima, aes(glucose, insulin, color=factor(test), shape=factor(test))) +
  geom_point() + theme_bw()
```
Try the plot with other variables. Find out correlations with `library(corrgram)`.
And then remove the correlated variables.
Find out finding correlated variables using `library(caret)`.

And now try for all variables.
```{r, warning=FALSE, message=FALSE, fig.width=10, fig.height=10}
library(GGally)
pima$test <- factor(pima$test)
ggpairs(pima)
```

## Origin of a car

In order to compare results please use the `Cars93` dataset. It has many interesting variables about brands of cars. One can use it to identify cars from USA cars from other countries.

```{r, message=FALSE, warning=FALSE}
library(party)
head(Cars93)

tr <- ctree(Origin~Manufacturer, data=Cars93)
plot(tr)

tr <- ctree(Origin~Type+Price+AirBags+DriveTrain+EngineSize, data=Cars93,
  controls=ctree_control(minsplit=0, mincriterion=0.9))
plot(tr)
```


# Overview of decision trees

## Tree structure

Tree is an undirected graph in which any two nodes are connected by exactly one path.
Rooted tree has a special node called root.
For classification trees variables names are used as node labels for internal nodes.
Classes or distributions of classes are used as labels in leaves.
Binary conditions correspond for egdes.


## Training of a tree

There are many algorithms for fitting of classification trees. The most popular ones covered and compared in http://www.stat.wisc.edu/~loh/treeprogs/guide/wires11.pdf.

The family of most popular algorithms (C4.5 / CART) are called Top-Down Induction of Decision Trees (TDIDT) and the name comes from the way general scheme for model building. 
The general algorithm for tree fitting is following:

1. Start with whole training dataset in the root node.
2. For each leaf in the tree find a best split (e.g. by exhaustive search) based on some purity measure (information gain, gain ratio).
3. Verify stopping criteria, if they are not met create two child nodes with subset of the original node (minimum size, maximum depth).
4. Repeat if there are nodes that can be further split.

5. (optionally) Prune back 
Small trees are better since we believe that the truth is simple (Occam’s Razor).


## Purity measures

* Gini impurity

Used in Classification and regression trees (CART)

$$
I_G(f) = \sum_{i=1}^m f_i(1-f_i) = 1 - \sum f_i^2
$$

* Information gain 

Used in C4.5

$$
I_E(f) = - \sum_{i=1}^g f_i \log_2 (f_i)
$$

Where $f_i$ if the frequency of label $i$ in a given node while $g$ stands for number of groups / classes.

## Predictions

For each observation follow the path from the root to leaves. Examine conditions in nodes.

## Implementations in R

There are many implementations in different packages. Here we are going to cover two most widely used, i.e. implementations from rpart and party.

### The rpart package

Use the `rpart` function to grow a tree. This function handles outcome variable of different classes, use `method="class"` for classification trees. 

```{r, message=FALSE, warning=FALSE, fig.width=10, fig.height=10}
library(rpart)

rtree <- rpart(test ~ insulin+glucose, data = pima, method="class")
rtree
plot(rtree)
text(rtree)
```

Two arguments: `params` and `control` control the way how the tree is build. For `params` use the `split` property to chose the measure that is optimized. 

```{r, message=FALSE, warning=FALSE}
rpart(test ~ insulin+glucose, data = pima,
    parms=list(split = "information"), method="class")
rpart(test ~ insulin+glucose, data = pima,
    parms=list(split = "gini"), method="class")
```

For `control` set the object created by `rpart.control`, that describes stopping and splitting criteria.  Following properties may be changed.

* minsplit - minimum number of observations to perform split of a node (default 30)
* minbucket - minimum number of observations for a node
* cp - complexity parameter (default 0.1)
* maxdepth - maximum depth of a tree (default 30)

A very large tree

```{r, message=FALSE, warning=FALSE, fig.width=10, fig.height=10}
tfit <- rpart(test ~ insulin+glucose, data = pima,
    parms=list(split = "gini"),
    control = rpart.control(cp=0))
tfit

table(real = pima$test,
      predicted = predict(tfit, data=pima, type = "class"))
mean(pima$test ==
       predict(tfit, data=pima, type = "class"))

plot(tfit)
text(tfit)
```

For deeper examination of a tree following functions are useful `printcp` and `plotcp`

```{r, warning=FALSE, message=FALSE}
plotcp(tfit)

printcp(tfit)

tfit <- rpart(test ~ insulin+glucose, data = pima,
    parms=list(split = "gini"),
    control = rpart.control(cp=0.1))

summary(tfit)
```

Use additional parameters of `plot()` and `text()` functions to tune the way in which tree is presented (add additional information about group sizes).

```{r}
plot(tfit, uniform=TRUE)
text(tfit, use.n=TRUE, all=TRUE, cex=.8)
```

Use the `prune()` function to prune / tune the tree. 

```{r}
prune(tfit, cp=0.02)
```


### The party package

The `party` package has a better visualisation for a tree. It has also a more general approach to handle different classes of outcome variable. See http://statmath.wu-wien.ac.at/~zeileis/papers/Hothorn+Hornik+Zeileis-2006.pdf for more details.

The most important functions from party package are: `ctree`, `predict`, `print`, `plot`.

Use `ctree` to fit a tree. The syntax is similar to `rpart`.

```{r, message=FALSE, warning=FALSE}
library(party)
pima$test <- factor(pima$test)

diabTree <- ctree(test ~ glucose+insulin, data = pima)

diabTree

plot(diabTree)
```

Roughly, the algorithm works as follows: 

1. Test the global null hypothesis of independence between any of the input variables and the response. 

STOP if this hypothesis cannot be rejected. Otherwise select the input variable with strongest association to the response. This association is measured by a p-value corresponding to a test for the partial null hypothesis of a single input variable and the response. 

2. Implement a binary split in the selected input variable. 
3. Recursively repeate steps 1. and 2.

Use the `predict()` function to classify data based on fitting model.

```{r}
table(real = pima$test,
      predicted = predict(diabTree))
```

Note that the `ctree` function works with continuous variables as well. 
Just as an example.

```{r, message=FALSE, warning=FALSE}
tmp <- ctree(glucose ~ insulin+test, data = pima)
tmp
plot(tmp)
```

### Control parameters

For `ctree` the default stop criteria is based on 1 - p-value for a selected test. 
P-values may be adjusted for number of applied tests (`testtype == "Bonferroni"`) or unjadusted (`testtype == "Univariate"`) or the test statistic may be used instead of p-value (`testtype == "Teststatistic"`).

Different test type are allowed. The parameter `testtype` based on general function for independence testing from package `coin`, namely `independence_test`.

Other optional control parameters may be set by `ctree_control` function. For example:

* mincriterion - split is made if p-value is smaller than 1-min criterion. For example `mincriterion=0.95` requires p-value smaller than 0.05. 
* minsplit - split is created if sum of weights for both child nodes is greater than minsplit
* minbucket - minimum sum of weights in terminal node

For example, a much larger tree

```{r, message=FALSE, warning=FALSE}
diabTree <- ctree(test ~ glucose+insulin, data = pima,
                    controls = ctree_control(mincriterion=0.1, minsplit=25, minbucket = 25))
diabTree
plot(diabTree)
```


## Summary

Pros

* White box model
* Simple to interpret
* No need for variable normalisation (use cutoffs)
* Handle numerical and categorical data

Cons

* Overfits


# Overview of Random Forests

Very good source of information about Random Forests

https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm


Key features
* Forest is composed out of trees, final results are calculated by voting (each tree has a single vote)
* Each tree is build based on a bootstrap sample (sample on the same size draw with replacement)
* Each tree is build based on a subset of m << p variables
* Each tree is grown to be the largest possible (a single tree will be over fitted)

Note that value m plays important role. Small m - reduced correlation between trees (better - lower error rate) and reduces strength of a tree (higher error rate). So there is something like 'optimal m'. Find with the use of OOB.


### Pros and cons

Pros

* Very good accuracy
* Can handle thousands of variables
* Gives estimates of variable importance

Cons

* A black box model


## Important concepts

### OOB (out-of-bag) error rate

For each tree around 1/3 of observations is not used for building the tree. Thus they may be used for assessment of tree performance. 

### Proximities 

For each pair of observations proximity is calculated as the chance that this pair falls to the same terminal node.

### Variable importance

For every tree one can calculate the added value in performance, i.e. how the tree is better than a random classifier. In order to get raw variable importance for a given variable sum up scores for all trees with given variable.

### Gini importance

As above but sum the improvements in gini coefficient.


## Implementations in R

The most widely know is in the `randomForest` package.

```{r}
library(randomForest)
ffit <- randomForest(test ~ .,   data=pima, importance = TRUE)
print(ffit) 
```

### Importance scores for variables.

```{r}
importance(ffit)
```

### Importance plot for variables.

```{r}
varImpPlot(ffit)
```

### Predictions

```{r}
head(predict(ffit, type="prob"))
```

# ROC (Receiver Operating Characteristics)

Classifiers that calculate scores / probabilities may be examined with the use of so called ROC. Let’s see an simple example for the trained random forest.

```{r, fig.width=8, fig.height=8}
prob <- predict(ffit, type="prob")[,2]

library(ROCR)
fit.pred = prediction(prob, pima$test)
fit.perf = performance(fit.pred,"tpr","fpr")
plot(fit.perf)
abline(a=0,b=1)
```

# The Homework

Download the dataset `student alcohol consumption` (find more about this dataset here: http://archive.ics.uci.edu/ml/datasets/STUDENT+ALCOHOL+CONSUMPTION). 
Train a decision tree and a random forest for different `m`.  Compare performance of both methods. 

#Useful links

https://www.r-bloggers.com/a-brief-tour-of-the-trees-and-forests/


